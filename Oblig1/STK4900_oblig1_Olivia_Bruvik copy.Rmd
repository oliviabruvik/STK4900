---
title: "STK4900 Mandatory Assignment 1"
author: "Olivia Beyer Bruvik"
date: "Due 3/12/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Problem 1)
```{r problem_1}

## read in no2.txt data
pollution_data <- read.table('https://www.uio.no/studier/emner/matnat/math/STK4900/v21/obliger/no2.txt', header=T)

## view start of data
head(pollution_data)

## view end of data
tail(pollution_data)

```

<br><br>

# Problem 1a)
The logarithms of the concentration of NO2 and number of cars have ranges of `r range(pollution_data$log.no2)[2] - range(pollution_data$log.no2)[1]` and `r range(pollution_data$log.cars)[2] - range(pollution_data$log.cars)[1]`, and interquartile ranges of `r IQR(pollution_data$log.no2)` and `r IQR(pollution_data$log.cars)`, respectively. 

<br> 

The data of the number of cars is more spread out than the data of the NO2 concentrations: the mean log-transformed concentrations of NO2 is `r mean(pollution_data$log.no2)` with a standard deviation of `r sd(pollution_data$log.no2)`, whereas the mean log-transformed number of cars per hour is `r mean(pollution_data$log.cars)` with a standard deviation of `r sd(pollution_data$log.cars)`. 

<br>

As can be seen in the scatterplot of car frequency vs. NO2 concentration, the log-transformed concentration of NO2 and the log-transformed number of cars per hour have a positive correlation, reflected in the Pearson's correlation coefficient of `r cor(pollution_data$log.no2, pollution_data$log.cars)`. The majority of the data points have high values of both log-transformed concentration of NO2 and log-transformed number of cars per hour. 


```{r problem_1a, out.width = "70%"}
## Main features and visualization of the log.no2 variable
summary(pollution_data$log.no2)
boxplot(pollution_data$log.no2,
        main = "The logarithm of the concentration of NO2",
        ylab = "Log-transformed concentration of NO2")

## Main features and visualization of the log.cars variable
summary(pollution_data$log.cars)
boxplot(pollution_data$log.cars,
        main = "The logarithm of the number of cars per hour",
        ylab = "Log-transformed number of cars per hour")

## Scatterplot of log.cars against log.no2
plot(x = pollution_data$log.cars,
     y = pollution_data$log.no2,
     xlab = "Log-transformed number of cars per hour",
     ylab = "Log-transformed concentration of NO2",
     main = "Car frequency vs. NO2 concentration")

## Pearson's correlation of log.cars and log.no2
cor(pollution_data$log.no2, pollution_data$log.cars)
```

<br>

# Problem 1b)
I fitted a simple linear model with log.no2 as the outcome and log.cars as the explanatory variable. The simple linear model is summarized and shown on the scatterplot of log.cars against log.no2 below. 

<br>

The estimated intercept and coefficient for log.cars are 1.233 and 0.354, respectively. The estimated intercept of 1.233 can be interpreted as the log-transformed concentration of NO2 when there is no traffic, measured by the log-transformed number of cars per hour. Conversely, the estimated log.cars coefficient of 0.354 can be interpreted as the average increase of log-transformed concentration of NO2 per unit increase in the log-transformed number of cars per hour, holding all else constant. 

<br> 

The coefficient of determination, or the multiple R-squared measure, is 0.262. This value reports that this model accounts for a proportion of 0.262 of the total variability in the log-transformed concentration of NO2, suggesting that the model has room for improvement. 

```{r problem_1b, out.width = "70%"}
## Simple linear model of log concentration of No2 explained by amount of traffic
fit.no2_traffic <- lm(log.no2 ~ log.cars, data = pollution_data)
summary(fit.no2_traffic)

## Scatterplot of log.cars against log.no2 with fitted line
plot(x = pollution_data$log.cars,
     y = pollution_data$log.no2,
     xlab = "Log-transformed number of cars per hour",
     ylab = "Log-transformed concentration of NO2",
     main = "Car frequency vs. NO2 concentration", 
     abline(fit.no2_traffic))

```
<br>

# Problem 1c)

The various residual plots below suggest that the model assumptions are reasonable. 

First, the Residuals vs. Fitted plot shows homoscedasticity, hence indicating that the true relationship is close to linear because the red line has very little curvature and is centered at Residuals = 0. 

The scale-location plot confirms this with the relatively horizontal red fitted line; however the line does tilt down towards larger fitted values, possibly because of the higher frequency of data points at high values, as discussed above.

In the quantile-quantile plot, the standardized residuals follow the straight dashed line, indicating that the residuals are approximately normally distributed. 

In the Residuals vs. Leverage plot, the datapoints labeled 104, 408 and 360 indicate datapoints with an absolute standardized residuals value above 2, suggesting that these datapoints may possibly be outliers and can be influential in the model.


```{r problem_1c, out.width = "50%"}
## various residual plots to judge model assumptions
plot(fit.no2_traffic)
```
<br>

# Problem 1d)

I fit various multiple regression models with log.no2 as the outcome, and the four other variables as explanatory. I found the 'best' model by looking at the coefficient of determination for each model. The model with the highest coefficient of determination (r^2 = 0.4833) that I could find included all four explanatory variables, but I log-transformed wind.speed. 

The other models that I tested and their respective coefficients of determination can be found in the appendix. 

```{r problem_1d, out.width = "70%"}
## multiple regression with log-transformed wind.speed
fit.multiple = lm(log.no2 ~ log.cars + temp + log(wind.speed) + hour.of.day, data = pollution_data)
summary(fit.multiple)

```

# Problem 1e)
I fitted a multiple regression model with log.no2 as the outcome and log.cars, temp, hour.of.day and log(wind.speed) as the explanatory variables. 

<br>

The estimated intercept is 1.071, indicating that the log-transformed concentration of NO2 is 1.071 when there is no traffic, a temperature of zero degrees and no wind at midnight. 

<br> 

The estimated coefficient for each explanatory variable can be interpreted as the average change of log-transformed concentration of NO2 per unit increase for the particular explanatory variable, holding all else constant. 

<br> 

The coefficients for temp, log(wind.speed) and hour.of.day are negative, suggesting that the log-transformed concentration decreases as these variables increase. Conversely, the positive log.cars coefficient demonstrates the positive relationship between log.cars and log.no2.

<br> 

The coefficient of determination, or the multiple R-squared measure, is 0.262. This value reports that this model accounts for a proportion of 0.262 of the total variability in the log-transformed concentration of NO2, suggesting that the model has room for improvement. 

<br>

The various residual plots below suggest that the model assumptions are reasonable, because the plots demonstrate homoscedasticity in the data and a general normal distribution of errors.

```{r problem_1e, out.width = "50%"}
## multiple regression with log-transformed wind.speed
fit.multiple = lm(log.no2 ~ log.cars + temp + log(wind.speed) + hour.of.day, data = pollution_data)
summary(fit.multiple)

## various plots to judge model assumptions
plot(fit.multiple)
```


# Problem 2
```{r problem_2}
## read in blood.txt data
bp_data <- read.table('https://www.uio.no/studier/emner/matnat/math/STK4900/v21/obliger/blood.txt', header=T)

## view start of data
head(bp_data)

## view end of data
tail(bp_data)

# Define the age groups as factors (categorical):
bp_data$age <- factor(bp_data$age)

```

# Problem 2a)

The blood pressure measurements have ranges of `r range(bp_data$Bloodpr[bp_data$age==1])[2] - range(bp_data$Bloodpr[bp_data$age==1])[1]`,  `r range(bp_data$Bloodpr[bp_data$age==2])[2] - range(bp_data$Bloodpr[bp_data$age==2])[1]` and  `r range(bp_data$Bloodpr[bp_data$age==3])[2] - range(bp_data$Bloodpr[bp_data$age==3])[1]` for age groups 1, 2 and 3, respectively.

<br> 

As can be seen in the boxplot, the mean bloodpressure increases as the age group, and by extension ages, increases. 

The bloodpressure data is also more spread out in higher age groups: the standard deviations for age group 1, 2 and 3 are `r sd(bp_data$Bloodpr[bp_data$age==1])`, `r sd(bp_data$Bloodpr[bp_data$age==2])` and `r sd(bp_data$Bloodpr[bp_data$age==3])`. 

```{r problem_2a}

## Main features and visualization of the data
# boxplot
boxplot(Bloodpr ~ age, 
        data = bp_data,
        main = "Blood pressure for each age group",
        xlab = "Age group",
        ylab = "Blood pressure")

# Summary all age groups
summary(bp_data)

# Summary age group 1
summary(bp_data$Bloodpr[bp_data$age==1])

# Summary age group 2
summary(bp_data$Bloodpr[bp_data$age==2])

# Summary age group 3
summary(bp_data$Bloodpr[bp_data$age==3])

```

# Problem 2b)
I ran a one-way ANOVA test below, with blood pressure as the outcome and age group as the explanatory variable. The assumptions involved in this test primarily involves that the data is normally distributed and that the age groups represent random samples. Furthermore, observations are assumed to be independent of each other. 

<br>

I am testing the null hypothesis that the mean blood pressure for each age group are all equal, and the alternative hypothesis that the mean blood pressure for each age group are not all equal. 


```{r problem_2b}
# One-way ANOVA test to see how blood pressure varies across age groups. 
aov.bp <- aov(Bloodpr~age, data = bp_data)
anova(aov.bp)

```
# Problem 2c)
Here, I formulated a regression model with age group as a categorical predictor variable. I used treatment contrast method and by default, the youngest age group was the reference. 

<br>

The results suggest that the null hypothesis can be rejected because of the large F value and t value in the one-way anova test and regression model. 

The blood pressure also evidently increases as the age group increases, evident in the coefficients.


```{r problem_2c}
# Regression with categorical predictor variables
lm.bp <- lm(Bloodpr~age, data = bp_data)
summary(lm.bp)

```


# Appendix

## Problem 1d)
```{r appendix_1d, out.width = "70%"}
## coefficient of determination of other multiple regression models
summary(lm(log.no2 ~ log.cars + temp + wind.speed + hour.of.day, data = pollution_data))[8]
summary(lm(log.no2 ~ log.cars + log(temp + 0.000001) + wind.speed + hour.of.day, data = pollution_data))[8]
summary(lm(log.no2 ~ log.cars + log(temp + 0.000001) + log(wind.speed) + hour.of.day, data = pollution_data))[8]
summary(lm(log.no2 ~ log.cars + I(temp^2) + log(wind.speed) + hour.of.day, data = pollution_data))[8]
summary(lm(log.no2 ~ log.cars + temp + I(wind.speed^2) + hour.of.day, data = pollution_data))[8]
summary(lm(log.no2 ~ log.cars + temp + log(wind.speed) + log(hour.of.day), data = pollution_data))[8]
summary(lm(log.no2 ~ log.cars + temp + I(wind.speed^2) + hour.of.day, data = pollution_data))[8]
pollution_data$temp_K <- pollution_data$temp - 273
summary(lm(log.no2 ~ log.cars + temp + I(wind.speed^2) + hour.of.day, data = pollution_data))[8]

```