---
title: "STK4900 Mandatory Assignment 1"
author: "Olivia Beyer Bruvik"
date: "Due 3/12/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(ggplot2)
```

# Problem 1)
```{r problem_1}

## read in no2.txt data
pollution_data <- read.table('https://www.uio.no/studier/emner/matnat/math/STK4900/v21/obliger/no2.txt', header=T)

## view start of data
head(pollution_data)

## view end of data
tail(pollution_data)

```

<br><br>

# Problem 1a)
The logarithms of the concentration of NO2 and number of cars have ranges of `r range(pollution_data$log.no2)[2] - range(pollution_data$log.no2)[1]` and `r range(pollution_data$log.cars)[2] - range(pollution_data$log.cars)[1]`, and interquartile ranges of `r IQR(pollution_data$log.no2)` and `r IQR(pollution_data$log.cars)`, respectively. 

<br> 

The data of the number of cars is more spread out than the data of the NO2 concentrations: the mean log-transformed concentrations of NO2 is `r mean(pollution_data$log.no2)` with a standard deviation of `r sd(pollution_data$log.no2)`, whereas the mean log-transformed number of cars per hour is `r mean(pollution_data$log.cars)` with a standard deviation of `r sd(pollution_data$log.cars)`. 

<br>

As can be seen in the scatterplot of car frequency vs. NO2 concentration, the log-transformed concentration of NO2 and the log-transformed number of cars per hour have a positive correlation, reflected in the Pearson's correlation coefficient of `r cor(pollution_data$log.no2, pollution_data$log.cars)`. The majority of the data points have high values of both log-transformed concentration of NO2 and log-transformed number of cars per hour. 


```{r problem_1a, out.width = "70%"}
## Main features and visualization of the log.no2 variable
summary(pollution_data$log.no2)
boxplot(pollution_data$log.no2,
        main = "The logarithm of the concentration of NO2",
        ylab = "Log-transformed concentration of NO2")

## Main features and visualization of the log.cars variable
summary(pollution_data$log.cars)
boxplot(pollution_data$log.cars,
        main = "The logarithm of the number of cars per hour",
        ylab = "Log-transformed number of cars per hour")

## Scatterplot of log.cars against log.no2
plot(x = pollution_data$log.cars,
     y = pollution_data$log.no2,
     xlab = "Log-transformed number of cars per hour",
     ylab = "Log-transformed concentration of NO2",
     main = "Car frequency vs. NO2 concentration")

## Pearson's correlation of log.cars and log.no2
cor(pollution_data$log.no2, pollution_data$log.cars)
```

<br>

# Problem 1b)
I fitted a simple linear model with log.no2 as the outcome and log.cars as the explanatory variable. The simple linear model is summarized and shown on the scatterplot of log.cars against log.no2 below. 

<br>

The estimated intercept and coefficient for log.cars are 1.233 and 0.354, respectively. The estimated intercept of 1.233 can be interpreted as the log-transformed concentration of NO2 when there is no traffic, measured by the log-transformed number of cars per hour. Conversely, the estimated log.cars coefficient of 0.354 can be interpreted as the average increase of log-transformed concentration of NO2 per unit increase in the log-transformed number of cars per hour, holding all else constant. 

<br> 

The coefficient of determination, or the R-squared measure, is 0.262. In this case, the R-squared measure is the squared value of the Pearson's correlation coefficient. The R-squared measure reports that this model accounts for a proportion of 0.262 of the total variability in the log-transformed concentration of NO2, suggesting that the model has room for improvement. One way to explore how we can improve the model is by analyzing additional predictors.

```{r problem_1b, out.width = "70%"}
## Simple linear model of log concentration of No2 explained by amount of traffic
fit.no2_traffic <- lm(log.no2 ~ log.cars, data = pollution_data)
summary(fit.no2_traffic)

## Scatterplot of log.cars against log.no2 with fitted line
plot(x = pollution_data$log.cars,
     y = pollution_data$log.no2,
     xlab = "Log-transformed number of cars per hour",
     ylab = "Log-transformed concentration of NO2",
     main = "Car frequency vs. NO2 concentration", 
     abline(fit.no2_traffic))

```
<br>

# Problem 1c)

The various residual plots below suggest that the model assumptions are reasonable. 

First, I checked for linearity and constant variance with a Residuals vs. Fitted plot. In the plot, the red line has very little curvature and is centered at Residuals = 0, demonstrating that the residuals are homoscedastic and that the true relationship between the independent (log.cars) and dependent (log.no2) variables is linear. The plot titled "Car frequency vs. NO2 concentration" above supports this observation by demonstrating that the datapoints center along the line added in the plot. Furthermore, the scale-location plot confirms these observations with the relatively horizontal red fitted line; however the line does tilt down towards larger fitted values, possibly because of the higher frequency of data points at high values, as discussed above. 

I then checked that the errors were normally distributed with a quantile-quantile plot. Here, we can see that the standardized residuals follow a straight dashed line, indicating that the residuals are approximately normally distributed. Furthermore, the distribution of the residuals follow a normal curve, as can be seen in the histogram titled "Normality in errors".

An interesting point to note involves the Residuals vs. Leverage plot. The datapoints labeled 104, 408 and 360 indicate datapoints with an absolute standardized residuals value above 2, suggesting that these datapoints may possibly be outliers and can be influential in the model.


```{r problem_1c, out.width = "50%"}
## various residual plots to judge model assumptions
plot(fit.no2_traffic)

hist(fit.no2_traffic$residuals,
     main="Normality in errors",
     xlab = "Residuals")
```
<br>

# Problem 1d)

I fit various multiple regression models with log.no2 as the outcome, and the four other variables as explanatory. I found the 'best' model by looking at the adjusted coefficient of determination for each model. The model with the highest adjusted coefficient of determination (adjusted r^2 = 0.4791) that I could find included all four explanatory variables, but I log-transformed wind.speed. 

The other models that I tested and their adjusted R-squared values can be found in the appendix. 

```{r problem_1d, out.width = "70%"}
## multiple regression with log-transformed wind.speed
fit.multiple = lm(log.no2 ~ log.cars + temp + log(wind.speed) + hour.of.day, data = pollution_data)
summary(fit.multiple)
```

# Problem 1e)
I fitted a multiple regression model with log.no2 as the outcome and log.cars, temp, hour.of.day and log(wind.speed) as the explanatory variables. 

<br>

The estimated intercept is 1.071, indicating that the log-transformed concentration of NO2 is 1.071 when there is no traffic, a temperature of zero degrees and no wind at midnight. 

<br> 

The estimated coefficient for each explanatory variable can be interpreted as the average change of log-transformed concentration of NO2 per unit increase for the particular explanatory variable, holding all else constant. 

<br> 

The coefficients for temp, log(wind.speed) and hour.of.day are negative, suggesting that the log-transformed concentration decreases as these variables increase. Conversely, the positive log.cars coefficient demonstrates the positive relationship between log.cars and log.no2.

<br> 

The coefficient of determination, or the multiple R-squared measure, is 0.262. This value reports that this model accounts for a proportion of 0.262 of the total variability in the log-transformed concentration of NO2, suggesting that the model has room for improvement. 

<br>

The various residual plots below suggest that the model assumptions are reasonable. First, the Residuals vs. Fitted plot indicates homoscedasticity in the data because of the limited curvature in the red line and alignment to the x-axis; however, the red line does deviate slightly from the dashed line at the x-axis for the smallest and largest fitted values. This suggests that the model is more accurate in the fitted values closer to the median value. This plot also indicates a reasonable amount of linearity in the model, supporting this assumption. Lastly, the general normal distribution of errors can be seen in the Normal Q-Q plot and in the histogram labeled, "Normality in errors". 


```{r problem_1e, out.width = "50%"}
## multiple regression with log-transformed wind.speed
fit.multiple = lm(log.no2 ~ log.cars + temp + log(wind.speed) + hour.of.day, data = pollution_data)
summary(fit.multiple)

## various plots to judge model assumptions
plot(fit.multiple)

## histogram to show normal distribution of errors
hist(fit.multiple$residuals,
     main="Normality in errors",
     xlab = "Residuals")
```


# Problem 2
```{r problem_2}
## read in blood.txt data
bp_data <- read.table('https://www.uio.no/studier/emner/matnat/math/STK4900/v21/obliger/blood.txt', header=T)

## view start of data
head(bp_data)

## view end of data
tail(bp_data)

# Define the age groups as factors (categorical):
bp_data$age <- factor(bp_data$age)

```

# Problem 2a)

The blood pressure measurements have ranges of `r range(bp_data$Bloodpr[bp_data$age==1])[2] - range(bp_data$Bloodpr[bp_data$age==1])[1]`,  `r range(bp_data$Bloodpr[bp_data$age==2])[2] - range(bp_data$Bloodpr[bp_data$age==2])[1]` and  `r range(bp_data$Bloodpr[bp_data$age==3])[2] - range(bp_data$Bloodpr[bp_data$age==3])[1]` for age groups 1, 2 and 3, respectively.

<br> 

As can be seen in the boxplot, the mean bloodpressure increases as the age group, and by extension ages, increases. 

The bloodpressure data is also more spread out in higher age groups: the standard deviations for age group 1, 2 and 3 are `r sd(bp_data$Bloodpr[bp_data$age==1])`, `r sd(bp_data$Bloodpr[bp_data$age==2])` and `r sd(bp_data$Bloodpr[bp_data$age==3])`. 

```{r problem_2a}

## Main features and visualization of the data
# boxplot
boxplot(Bloodpr ~ age, 
        data = bp_data,
        main = "Blood pressure for each age group",
        xlab = "Age group",
        ylab = "Blood pressure")

# Summary all age groups
summary(bp_data)

# Summary age group 1
summary(bp_data$Bloodpr[bp_data$age==1])

# Summary age group 2
summary(bp_data$Bloodpr[bp_data$age==2])

# Summary age group 3
summary(bp_data$Bloodpr[bp_data$age==3])

```

# Problem 2b)


I next fit an analysis of variance model to analyze how blood pressure varies across the three groups, and by extension, age. I ran the one-way ANOVA test below, which fits an analysis of variance model by calls to the lm() function in R with blood pressure as the outcome and age group as the explanatory variable. The parameters of the model are $\mu$~1~, $\mu$~2~, and $\mu$~3, which represent the mean blood pressure for subjects aged 30–45 years (group 1), 46 – 59 years (group 2), and ages 60 –75 years (group 3), respectively. 

The assumptions involved in this test primarily involves that the data is normally distributed and that the age groups represent random samples. Furthermore, observations are assumed to be independent of each other. This assumption is satisfied by the data, because blood pressure in different people from a random sample is uncorrelated, and furthermore, the groups are independent. Errors are also assumed to be independent and normally distributed, because this ANOVA test involves a fixed effects model. 

<br>

I am testing the null hypothesis that the mean blood pressure for each age group are all equal: H~0~ = $\mu$~1~ = $\mu$~2~ = $\mu$~3~, where $\mu$ denotes the mean blood pressure for group k. The alternative hypothesis that the mean blood pressure for each age group are not all equal. 

The results can be seen in the ANOVA table below, where bloodpressure is the response variable. In the table, Df, Sum Sq, Mean Sq, F value, and Pr(>F) give the degrees of freedom, sum of squares, mean squares, F values and p-values, respectively. The first source of variation in the table is age, which has two degrees of freedom, a between age group sum of squares of 6535.4. Next, the mean squares is found by dividing the sum of squares by the degrees of freedom for that source of variation, which gives a value of 3267.7. The F value is 6.4686, and is used to calculate the p-value, 0.004263. This p-value indicates that there is a 0.004263 probability of obtaining at least as extreme results as those observed, given that the null hypothesis is correct. This p-value is significant (<0.05) and also suggests that the null hypothesis can be rejected in confidence. That is, these results indicate that the mean blood pressure is not equal across age groups. Residuals act as another source of variability with 33 degrees of freedom. The values for the sum of squares and mean squares are 16670.2 and 505.2, respectively.


```{r problem_2b}
# One-way ANOVA test to see how blood pressure varies across age groups. 
aov.bp <- aov(Bloodpr~age, data = bp_data)
anova(aov.bp)

```
# Problem 2c)
Here, I formulated a regression model with age group as a categorical predictor variable. I used the treatment contrast method, with the first age group (ages 30–45) as the reference and therefore set to 0. The intercept of the model gives the estimated average blood pressure for the baseline group - age group 1 - which is equal to 122.167. This intercept has a p-value, given in the Pr(>|t|) column, of < 2e-16, which is statistically significant. 

The second coefficient, 16.917, gives the average change in blood pressure from that of group 1 to group 2, holding all else constant. This suggest that on average, subjects in group 2 had blood pressure that is 16.917 units higher than that in group 1; however, this coefficient has a p-value of 0.07423, which is not significant. Therefore, the null hypothesis cannot be rejected, and a difference in mean blood pressure accross group 1 and group 2 cannot be determined. 


The third coefficient, 33.000, can be interpreted in the same manner as the previous coefficient. That is, holding all else constant, the mean blood pressure of subjects in group 3 was measured to be 33.000 units higher than the mean blood pressure of subjects in group 1. This coefficient has a p-value of 0.00104, which is statistically significant. The null hypothesis can therefore be rejected, and the mean blood pressure can be said to differ across group 1 and 2. 

The reason that there is a signficant difference in mean blood pressure in groups 1 and 3, but not groups 1 and 2 may be attributed to the smaller difference in age in the latter set of groups, and therefore that any change in blood pressure becomes less significant. This may suggests that blood pressure may increases with age, but more data and statistical tests would be necessary to support this statement.


```{r problem_2c}
# Regression with categorical predictor variables
lm.bp <- lm(Bloodpr~age, data = bp_data)
summary(lm.bp)

## mean blood pressure in group 1
mean(bp_data$Bloodpr[bp_data$age==1]) ## consistent with linear lm.bp intercept

```


# Appendix

## Problem 1d)
```{r appendix_1d, out.width = "70%"}
## coefficient of determination of other multiple regression models
summary(lm(log.no2 ~ log.cars + temp + wind.speed + hour.of.day, data = pollution_data))[9]
summary(lm(log.no2 ~ log.cars + log(temp + 0.000001) + wind.speed + hour.of.day, data = pollution_data))[9]
summary(lm(log.no2 ~ log.cars + log(temp + 0.000001) + log(wind.speed) + hour.of.day, data = pollution_data))[9]
summary(lm(log.no2 ~ log.cars + I(temp^2) + log(wind.speed) + hour.of.day, data = pollution_data))[9]
summary(lm(log.no2 ~ log.cars + temp + I(wind.speed^2) + hour.of.day, data = pollution_data))[9]
summary(lm(log.no2 ~ log.cars + temp + log(wind.speed) + log(hour.of.day), data = pollution_data))[9]
summary(lm(log.no2 ~ log.cars + temp + I(wind.speed^2) + hour.of.day, data = pollution_data))[9]
pollution_data$temp_K <- pollution_data$temp - 273
summary(lm(log.no2 ~ log.cars + temp + I(wind.speed^2) + hour.of.day, data = pollution_data))[9]

```